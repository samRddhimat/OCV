{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1304a23-f66e-4bd6-8569-26083cddcab2",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Image Enhancement</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e98d64-45bb-499d-97a3-435c6711b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as mplt\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eef1a4-586a-4733-a1ec-3df4e9618dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bgr = cv2.imread(\"D:/Repository/OCV/04_image_enhancement/04_New_Zealand_Coast.jpg\",cv2.IMREAD_COLOR)\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "Image(filename=\"D:/Repository/OCV/04_image_enhancement/04_New_Zealand_Coast.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fde9c-81c4-473f-a49d-964c8643bc15",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Addition or Brightness</code>\n",
    "\n",
    "Simple addition of images. This results in increasing or decreasing the brightness of the image since we are eventually increasing or decreasing the intensity values of each pixel by the same amount. So, this will result in a global increase/decrease in brightness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f02a9e0-a1ac-4644-b97d-45263c035c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.ones(img_rgb.shape,dtype=\"uint8\") * 50\n",
    "\n",
    "img_rgb_brighter = cv2.add(img_rgb, matrix)\n",
    "img_rgb_darker = cv2.subtract(img_rgb,matrix)\n",
    "\n",
    "# img_rgb_darker_2 =cv2.subtract(img_rgb,img_rgb_darker)\n",
    "fig,axs = mplt.subplots(1,3,figsize=(18,5))\n",
    "\n",
    "axs[0].set_title(\"Darker\")\n",
    "axs[0].imshow(img_rgb_darker)\n",
    "axs[0].title.set_size(10)\n",
    "\n",
    "axs[1].set_title(\"Original\")\n",
    "axs[1].imshow(img_rgb)\n",
    "axs[1].title.set_size(10)\n",
    "\n",
    "axs[2].set_title(\"Brighter\")\n",
    "axs[2].imshow(img_rgb_brighter)\n",
    "axs[2].title.set_size(10)\n",
    "\n",
    "mplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2aa9c-c95d-40a8-98bb-9cd4338d1487",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Multiplication or Contrast</code>\n",
    "\n",
    "Just like addition can result in brightness change, multiplication can be used to improve the contrast of the image.\n",
    "\n",
    "Contrast is the difference in the intensity values of the pixels of an image. Multiplying the intensity values with a constant can make the difference larger or smaller ( if multiplying factor is < 1 ).\n",
    "\n",
    "NB: Contrast defined as difference in intensity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028e0d4-0810-4f34-ab37-afac165639bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.ones(img_rgb.shape) * 0.8\n",
    "matrix2 = np.ones(img_rgb.shape) * 1.2\n",
    "\n",
    "img_rgb_darker = np.uint8(cv2.multiply(np.float64(img_rgb), matrix1))\n",
    "img_rgb_brighter = np.uint8(cv2.multiply(np.float64(img_rgb), matrix2))\n",
    "\n",
    "fig,axs = mplt.subplots(1,3,figsize=(18,5))\n",
    "axs[0].set_title(\"Lower Contrast\")\n",
    "axs[0].imshow(img_rgb_darker)\n",
    "axs[0].title.set_size(10)\n",
    "\n",
    "axs[1].set_title(\"Original\")\n",
    "axs[1].imshow(img_rgb)\n",
    "axs[1].title.set_size(10)\n",
    "\n",
    "cv2.circle(img_rgb_brighter, (110,70), 80, (255, 255, 55), thickness=5, lineType=cv2.LINE_AA);\n",
    "\n",
    "cv2.circle(img_rgb_brighter, (210,300), 80, (255, 0, 250), thickness=5, lineType=cv2.LINE_AA);\n",
    "cv2.arrowedLine(img_rgb_brighter, (210,300), (210,300), (255, 0, 250), thickness=5, line_type=8);\n",
    "\n",
    "axs[2].set_title(\"Higher Contrast\")\n",
    "axs[2].imshow(img_rgb_brighter)\n",
    "axs[2].title.set_size(10)\n",
    "\n",
    "\n",
    "mplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b3c83-4a44-49f8-8418-da8ff8d7e9c5",
   "metadata": {},
   "source": [
    "### <code style=\"background:green;color:white\">Weird colors</code>\n",
    "\n",
    "What happened?\n",
    "Can you see the weird colors in some areas of the image after multiplication?\n",
    "\n",
    "The issue is that after multiplying, the values which are already high, are becoming greater than 255. Thus, the overflow issue. How do we overcome this?\n",
    "\n",
    "## <code style=\"background:red;color:white\">Handling overflow using np.clip</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36159b69-9343-4ec3-b4b5-053fe18c6c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.ones(img_rgb.shape) * 0.8\n",
    "matrix2 = np.ones(img_rgb.shape) * 1.2\n",
    "\n",
    "img_rgb_lower = np.uint8(cv2.multiply(np.float64(img_rgb), matrix1))\n",
    "img_rgb_higher = np.uint8(np.clip(cv2.multiply(np.float64(img_rgb), matrix2),0,255))\n",
    "\n",
    "fig,axs = mplt.subplots(1,3,figsize=(18,5))\n",
    "axs[0].set_title(\"Lower Contrast\")\n",
    "axs[0].imshow(img_rgb_darker)\n",
    "axs[0].title.set_size(10)\n",
    "\n",
    "axs[1].set_title(\"Original\")\n",
    "axs[1].imshow(img_rgb)\n",
    "axs[1].title.set_size(10)\n",
    "\n",
    "axs[2].set_title(\"Higher Contrast\")\n",
    "axs[2].imshow(img_rgb_higher)\n",
    "axs[2].title.set_size(10)\n",
    "\n",
    "\n",
    "mplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49660ba4-df32-4b7a-81b2-d948e8f1ecf9",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Image Thresholding</code>\n",
    "\n",
    "Binary Images have a lot of use cases in Image Processing. One of the most common use cases is that of creating masks. Image Masks allow us to process on specific parts of an image keeping the other parts intact. Image Thresholding is used to create Binary Images from grayscale images. You can use different thresholds to create different binary images from the same original image.\n",
    "\n",
    "#### <code style=\"background:red;color:white\">Function Syntax</code>\n",
    "\n",
    "<b>retval, dst = cv2.threshold( src, thresh, maxval, type[, dst] )</b>\n",
    "<ol><li><b>dst:</b> The output array of the same size and type and the same number of channels as <b>src</b>.</li></ol>\n",
    "\n",
    "The function has 4 required arguments:\n",
    "<ol type=\"1\">\n",
    "<li>src: input array (multiple-channel, 8-bit or 32-bit floating point).</li>\n",
    "\n",
    "<li>thresh: threshold value.</li>\n",
    "\n",
    "<li>maxval: maximum value to use with the THRESH_BINARY and THRESH_BINARY_INV thresholding types.</li>\n",
    "\n",
    "<li>type: thresholding type (see ThresholdTypes).\n",
    "</li></ol>\n",
    "\n",
    "\n",
    "#### <code style=\"background:red;color:white\">Function Syntax</code>\n",
    "<b>dst = cv.adaptiveThreshold( src, maxValue, adaptiveMethod, thresholdType, blockSize, C[, dst] )</b>\n",
    "dst Destination image of the same size and the same type as src.\n",
    "\n",
    "The function has 6 required arguments:\n",
    "\n",
    "<ol type=\"1\">\n",
    "<li>src: Source 8-bit single-channel image.</li>\n",
    "\n",
    "<li>maxValue: Non-zero value assigned to the pixels for which the condition is satisfied</li>\n",
    "\n",
    "<li>adaptiveMethod: Adaptive thresholding algorithm to use, see AdaptiveThresholdTypes. The BORDER_REPLICATE | BORDER_ISOLATED is used to process boundaries.</li>\n",
    "\n",
    "<li>thresholdType: Thresholding type that must be either THRESH_BINARY or THRESH_BINARY_INV, see ThresholdTypes.</li>\n",
    "\n",
    "<li>blockSize: Size of a pixel neighborhood that is used to calculate a threshold value for the pixel: 3, 5, 7, and so on.</li>\n",
    "\n",
    "<li>C: Constant subtracted from the mean or weighted mean (see the details below). Normally, it is positive but may be zero or negative as well.</li>\n",
    "\n",
    "</ol>\n",
    "<a href=\"https://docs.opencv.org/4.5.1/d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57\">Documentation link</a> <br>\n",
    "<a href=\"https://docs.opencv.org/4.5.1/d7/d4d/tutorial_py_thresholding.html\">Documentation link</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc052b-f774-4972-a935-e1f59e3d2791",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_read = cv2.imread(\"D:/Repository/OCV/04_image_enhancement/04_building-windows.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# print(img_read )\n",
    "retval, img_thresh = cv2.threshold(img_read, 100, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# mplt.subplot(122);mplt.imshow(img_thresh, cmap=\"gray\");mplt.title(\"Thresholded\")\n",
    "\n",
    "fig,axs = mplt.subplots(1,2,figsize=(18,5))\n",
    "\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[0].imshow(img_read, cmap=\"gray\")\n",
    "axs[0].title.set_size(10)\n",
    "\n",
    "axs[1].set_title(\"Original\")\n",
    "axs[1].imshow(img_thresh,cmap=\"gray\")\n",
    "axs[1].title.set_size(10)\n",
    "\n",
    "mplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc71a24-2aa7-4957-b943-571760456404",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Application: Sheet Music Reader</code>\n",
    "\n",
    "\n",
    "<pre>Suppose you wanted to build an application that could read (decode) sheet music. This is similar to Optical Character Recognigition (OCR) for text documents where the goal is to recognize text characters. In either application, one of the first steps in the processing pipeline is to isolate the important information in the image of a document (separating it from the background). This task can be accomplished with thresholding techniques. Let's take a look at an example.</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919b152-74de-4a59-85f7-9fb9f4ae1266",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_read = cv2.imread(\"D:/Repository/OCV/04_image_enhancement/04_Piano_Sheet_Music.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Perform global thresholding\n",
    "retval, img_thresh_gbl_1 = cv2.threshold(img_read, 50, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Perform global thresholding\n",
    "retval, img_thresh_gbl_2 = cv2.threshold(img_read, 130, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Perform adaptive thresholding\n",
    "img_thresh_adp = cv2.adaptiveThreshold(img_read, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 7)\n",
    "\n",
    "fig,axs = mplt.subplots(1,4,figsize=(18,5))\n",
    "\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[0].imshow(img_read,cmap=\"gray\")\n",
    "axs[0].title.set_size(10)\n",
    "\n",
    "axs[1].set_title(\"Thresholded (global: 50)\")\n",
    "axs[1].imshow(img_thresh_gbl_1,cmap=\"gray\")\n",
    "axs[1].title.set_size(10)\n",
    "\n",
    "axs[2].set_title(\"Thresholded (global: 130)\")\n",
    "axs[2].imshow(img_thresh_gbl_2,cmap=\"gray\")\n",
    "axs[2].title.set_size(10)\n",
    "\n",
    "axs[3].set_title(\"Thresholded (adaptive)\")\n",
    "axs[3].imshow(img_thresh_adp,cmap=\"gray\")\n",
    "axs[3].title.set_size(10)\n",
    "\n",
    "mplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbede5e2-3e05-41ff-9859-f35da593befe",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Application: Sheet Music Reader</code>\n",
    "\n",
    "\n",
    "<pre>computes bitwise conjunction of the two arrays (dst = src1 & src2) Calculates the per-element bit-wise conjunction of two arrays or an array and a scalar.\n",
    "\n",
    "Others include: cv2.bitwise_or(), cv2.bitwise_xor(), cv2.bitwise_not()\n",
    "\n",
    "    The function cv::bitwise_and calculates the per-element bit-wise logical conjunction for: Two arrays when src1 and src2 have the same size:\n",
    "\n",
    "dst(I)=src1(I)‚àßsrc2(I)if mask(I)‚â†0\n",
    "\n",
    "dst = cv2.bitwise_and( src1, src2[, dst[, mask]] )\n",
    "    \n",
    "In case of floating-point arrays, their machine-specific bit representations (usually IEEE754-compliant) are used for the operation. In case of multi-channel arrays, each channel is processed independently. In the second and third cases above, the scalar is first converted to the array type.\n",
    "\n",
    "Parameters\n",
    "src1\tfirst input array or a scalar.\n",
    "src2\tsecond input array or a scalar.\n",
    "dst\toutput array that has the same size and type as the input arrays.\n",
    "mask\toptional operation mask, 8-bit single channel array, that specifies elements of the output array to be changed.\n",
    "\n",
    "</pre>\n",
    "\n",
    "<a href=\"https://docs.opencv.org/4.5.1/d0/d86/tutorial_py_image_arithmetics.html\">Image Arithmetic</a>\n",
    "\n",
    "<a href=\"https://docs.opencv.org/4.5.0/d2/de8/group__core__array.html#ga60b4d04b251ba5eb1392c34425497e14\">Bitwise_and()</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb848b-a750-4313-892d-aa76e45d8f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rec = cv2.imread(\"D:/Repository/OCV/04_image_enhancement/04_rectangle.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "img_cir = cv2.imread(\"D:/Repository/OCV/04_image_enhancement/04_circle.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "fig,axs = mplt.subplots(1,2,figsize=(18,5))\n",
    "\n",
    "axs[0].imshow(img_rec,cmap=\"gray\")\n",
    "axs[0].title.set_size(10)\n",
    "\n",
    "axs[1].imshow(img_cir,cmap=\"gray\")\n",
    "axs[1].title.set_size(10)\n",
    "\n",
    "mplt.show()\n",
    "\n",
    "print(img_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b9db6-2e3f-4b6d-ba2c-eb9e80ace1b0",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Bitwise AND Operator</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c42441-8ca1-4f44-bab0-102342631d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cv2.bitwise_and(img_rec,img_cir,None)\n",
    "mplt.imshow(result,cmap=\"gray\")\n",
    "mplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785119b4-5185-486e-aa9c-50a961fcb69a",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Bitwise OR Operator</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29899b8e-e8fc-4414-be59-14ade190630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cv2.bitwise_or(img_rec,img_cir,None)\n",
    "mplt.imshow(result,cmap=\"gray\")\n",
    "mplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ba3cce-51ec-45d6-804e-deef07a3ae38",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Bitwise XOR Operator</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d679fd9f-1eff-4c71-9116-54f65780f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cv2.bitwise_xor(img_rec,img_cir,None)\n",
    "mplt.imshow(result,cmap=\"gray\")\n",
    "mplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e6b27-50c3-499d-981f-93cf0671c605",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Bitwise Not Operator</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a098ba82-6a49-4549-a48c-182697dd1e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cv2.bitwise_not(img_cir,img_rec,None)\n",
    "mplt.imshow(result,cmap=\"gray\")\n",
    "mplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4745445-44ee-4166-834e-e7fe524916d3",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Application: Logo Manipulation</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53ca78-d767-4984-8302-98d5f182a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='D:/Repository/OCV/04_image_enhancement/04_Logo_Manipulation.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf3493-1e31-4ce1-bf60-7c71a467c6d2",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Read Foreground image</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a66b82-0ebd-4894-85ed-70e0868cab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bgr = cv2.imread(\"D:/Repository/OCV/04_image_enhancement/04_coca-cola-logo.png\")\n",
    "img_rgb = cv2.cvtColor(img_bgr,cv2.COLOR_BGR2RGB)\n",
    "mplt.imshow(img_rgb)\n",
    "mplt.show()\n",
    "\n",
    "logo_w,logo_h,_= img_rgb.shape\n",
    "print(f\"Shape of img_rgb is{img_rgb.shape}, individually extracted as {logo_w},{logo_h},{_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32762d5-5748-484e-bf33-5a0eed549a62",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Read Background image</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e52a11-82e4-453d-8f5c-42028833d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in image of color cheackerboad background\n",
    "img_background_bgr = cv2.imread(\"D:/Repository/OCV/04_image_enhancement/04_checkerboard_color.png\")\n",
    "img_background_rgb = cv2.cvtColor(img_background_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Set desired width (logo_w) and maintain image aspect ratio\n",
    "aspect_ratio = logo_w / img_background_rgb.shape[1]\n",
    "dim = (logo_w, int(img_background_rgb.shape[0] * aspect_ratio))\n",
    "\n",
    "# Resize background image to sae size as logo image\n",
    "img_background_rgb = cv2.resize(img_background_rgb, dim, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "mplt.imshow(img_background_rgb)\n",
    "mplt.show()\n",
    "print(img_background_rgb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d231240-3574-4e80-8c7b-6761c6ded4c4",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Create Mask for original Image</code>\n",
    "\n",
    "<pre>\n",
    "    _, binary = cv2.threshold(gray_img, thresh_val, max_val, cv2.THRESH_BINARY)\n",
    "    Thresholding is used to convert a grayscale (or color) image into a binary image ‚Äî usually black and white ‚Äî based on intensity.\n",
    "    Every pixel above thresh_val becomes white\n",
    "    Every pixel below becomes black    \n",
    "\n",
    "    üß† Why Use Thresholding?\n",
    "            Thresholding is helpful for:\n",
    "            \n",
    "            üîç Simplifying the image (removing shades, only keeping edges/structures)\n",
    "            \n",
    "            üìê Contour/shape detection\n",
    "            \n",
    "            ‚úÇÔ∏è Masking objects\n",
    "            \n",
    "            üß† Feeding to OCR or classification models\n",
    "            \n",
    "            ‚öôÔ∏è Preparing images for morphological operations (like closing gaps or removing noise)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b48ee-0608-4579-b27f-f879f0771c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray = cv2.cvtColor(img_rgb,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Apply global thresholding to creat a binary mask of the logo\n",
    "retval, img_mask = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "mplt.imshow(img_mask, cmap=\"gray\")\n",
    "mplt.show()\n",
    "print(img_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953974a0-3297-4753-b19d-8c08733bf492",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Invert the Mask</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d64c8b-a4d6-4968-bb0e-c491f0a290ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an inverse mask\n",
    "img_mask_inv = cv2.bitwise_not(img_mask)\n",
    "mplt.imshow(img_mask_inv, cmap=\"gray\")\n",
    "mplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb20b3-80df-41a3-92b7-b5e2d836dd1b",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Apply background on the Mask</code>\n",
    "\n",
    "<pre>\n",
    "    Case\t                                  Behavior\n",
    "bitwise_and(image, image)\t            Returns the original image\n",
    "bitwise_and(image, image, mask=mask)\tReturns only the parts of image where mask == 255\n",
    "bitwise_and(img1, img2)\t                Returns overlapping bright areas (logical AND)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fa8cd3-ab14-4c0b-8884-93905ccdeb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create colorful background \"behind\" the logo lettering\n",
    "img_background = cv2.bitwise_and(img_background_rgb, img_background_rgb, mask=img_mask)\n",
    "mplt.imshow(img_background)\n",
    "mplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5e7740-e386-45bd-baae-e790ebd333f9",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Isolate foreground from image</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d9a9d-3570-40c0-8236-c3c6abe164a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate foreground (red from original image) using the inverse mask\n",
    "img_foreground = cv2.bitwise_and(img_rgb, img_rgb, mask=img_mask_inv)\n",
    "mplt.imshow(img_foreground)\n",
    "mplt.show()\n",
    "\n",
    "print(img_rgb.shape, img_mask_inv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc8b4e9-4799-46ba-88f1-4d3272c1b4e0",
   "metadata": {},
   "source": [
    "## <code style=\"background:red;color:white\">Result: Merge Foreground and Background</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74869f1-2991-4ef8-b4bf-5c655c288038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add the two previous results obtain the final result\n",
    "result = cv2.add(img_background, img_foreground)\n",
    "mplt.imshow(result)\n",
    "mplt.show()\n",
    "cv2.imwrite(\"logo_final.png\", result[:, :, ::-1])\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61812ba2-fefc-4907-ad6b-d2bc7c379b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{result[0,10]},{result[0,10,0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e28e7-e112-4779-8508-47d6d0ebb80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_word(sentence):\n",
    "    return max(sentence.split(),key=len,default=\"\"), min(sentence.split(),key=len,default=\"\"), (max(sentence.split(),key=len,default=\"\") + min(sentence.split(),key=len,default=\"\"))\n",
    "\n",
    "print(type(find_longest_word(\"Feeding to OCR or classification models\")))\n",
    "print(find_longest_word(\"Feeding to OCR or classification models\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
